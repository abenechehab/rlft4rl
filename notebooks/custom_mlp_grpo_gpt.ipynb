{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2248c26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data_2/abenechehab/micromamba/envs/rlft4rl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-16 09:46:49,508] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data_2/abenechehab/micromamba/envs/rlft4rl/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/mnt/data_2/abenechehab/micromamba/envs/rlft4rl/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 09:46:50 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:46:50,885\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import minari\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM, PretrainedConfig, PreTrainedModel\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e8599",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9627134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPConfig(PretrainedConfig):\n",
    "    model_type = \"halfcheetah-mlp\"\n",
    "\n",
    "    def __init__(self, input_dim=17, output_dim=6, hidden_sizes=[64, 64], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "class MLPPolicy(PreTrainedModel):\n",
    "    config_class = MLPConfig  # enable AutoModel support\n",
    "    base_model_prefix = \"halfcheetah-mlp\"\n",
    "\n",
    "    def __init__(self, config: MLPConfig):\n",
    "        super().__init__(config)\n",
    "        layers = []\n",
    "        dims = [config.input_dim] + config.hidden_sizes + [config.output_dim]\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            if i < len(dims) - 2:\n",
    "                layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.mean = nn.Sequential(nn.Linear(dims[-1], config.output_dim), nn.ReLU())\n",
    "        self.log_std = nn.Sequential(nn.Linear(dims[-1], config.output_dim), nn.ReLU())\n",
    "        # For stochastic policy: a trainable log_std parameter per action\n",
    "        # self.log_std = nn.Parameter(torch.zeros(config.output_dim))\n",
    "\n",
    "    def forward(self, state, num_logits_to_keep=None):\n",
    "        # state shape: (batch_size, 17)\n",
    "        if state.shape[-1] == self.config.output_dim:\n",
    "            return state\n",
    "        elif state.shape[-1] == self.config.input_dim:\n",
    "            state = state.to(self.net[0].weight.device)\n",
    "            common = self.net(state)  # (batch_size, 6)\n",
    "            mean = self.mean(common)\n",
    "            std = torch.exp(self.log_std(common))\n",
    "            # dist = torch.distributions.Normal(mean, std)\n",
    "            # action = dist.rsample()     \n",
    "            # return action\n",
    "            return mean, std\n",
    "        else:    \n",
    "            raise ValueError(\n",
    "                f\"Expected input dimension {self.config.input_dim}, but got {state.shape[-1]}\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def generate(self, inputs=None, num_return_sequences=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate action samples given a batch of inputs (states).\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): input tensor of shape [batch_size, obs_dim]\n",
    "            num_return_sequences (int): number of action samples per input\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: tensor of shape [batch_size * num_return_sequences, action_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        mean, std = self.forward(kwargs[\"input_ids\"].float())\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "\n",
    "        actions = []\n",
    "        for _ in range(num_return_sequences):\n",
    "            sampled = dist.sample()\n",
    "            actions.append(sampled)\n",
    "\n",
    "        # shape: [num_return_sequences, batch_size, action_dim] â†’ [batch_size * num_return_sequences, action_dim]\n",
    "        all_actions = torch.cat(actions, dim=0)\n",
    "        return all_actions\n",
    "    \n",
    "\n",
    "# Register the configuration\n",
    "AutoConfig.register(\"halfcheetah-mlp\", MLPConfig)\n",
    "# Register the model\n",
    "AutoModel.register(MLPConfig, MLPPolicy)\n",
    "\n",
    "def create_mlp_model(obs_dim, action_dim, hidden_dims=None, **kwargs):\n",
    "    config = MLPConfig(\n",
    "        obs_dim=obs_dim, action_dim=action_dim, hidden_dims=hidden_dims, **kwargs\n",
    "    )\n",
    "    return MLPPolicy(config)\n",
    "\n",
    "\n",
    "def load_mlp_model(model_path):\n",
    "    config = MLPConfig.from_pretrained(model_path)\n",
    "    return MLPPolicy.from_pretrained(model_path, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a4a10",
   "metadata": {},
   "source": [
    "### Tokenizer (dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fd8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class MLPTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, obs_dim, **kwargs):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.eos_token = 0\n",
    "        self.eos_token_id = 0\n",
    "        self.pad_token = 0\n",
    "        self.pad_token_id = 0\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # extract observation from str and make it array\n",
    "        obs_list = text.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n",
    "        obs_list = [float(obs) for obs in obs_list if obs.strip()]\n",
    "        if len(obs_list) != self.obs_dim:\n",
    "            raise ValueError(f\"Expected {self.obs_dim} observations, got {len(obs_list)}\")\n",
    "        # Convert each element to its own numpy array\n",
    "        return [str(elem) for elem in obs_list]\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return np.array(token).astype(np.float32)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return 1  # Dummy value\n",
    "    \n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str, ...]:\n",
    "        return ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa9806",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ecd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"mujoco/halfcheetah/medium-v0\"\n",
    "seed = 7\n",
    "dataset_size = 5\n",
    "\n",
    "dataset = minari.load_dataset(dataset_id, download=True)\n",
    "dataset.set_seed(seed=seed)\n",
    "\n",
    "examples = []\n",
    "for i, ep in enumerate(dataset):\n",
    "    for t in range(ep.observations.shape[0] - 1):\n",
    "        examples.append(\n",
    "            {\n",
    "                \"prompt\": f\"{ep.observations[t].tolist()}\",\n",
    "                \"state\": ep.observations[t].tolist(),\n",
    "                \"action\": ep.actions[t].tolist(),\n",
    "                \"reward\": float(ep.rewards[t]),\n",
    "            }\n",
    "        )\n",
    "        if i==0 and t==0:\n",
    "            input_dim = len(ep.observations[t])\n",
    "            output_dim = len(ep.actions[t])\n",
    "    if i >= dataset_size:\n",
    "        break\n",
    "\n",
    "hf_dataset = Dataset.from_list(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02725c2",
   "metadata": {},
   "source": [
    "### Custom GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f77309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Union\n",
    "from datasets import Dataset, IterableDataset\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "from transformers.utils import is_peft_available\n",
    "\n",
    "from trl.models.utils import unwrap_model_for_generation\n",
    "\n",
    "\n",
    "if is_peft_available():\n",
    "    from peft import PeftConfig\n",
    "\n",
    "# What we call a reward function is a callable that takes a list of prompts and completions and returns a list of\n",
    "# rewards. When it's a string, it's a model ID, so it's loaded as a pretrained model.\n",
    "RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9ab480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRPOTrainer(GRPOTrainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Union[str, PreTrainedModel],\n",
    "        reward_funcs: Union[RewardFunc, list[RewardFunc]],\n",
    "        args: GRPOConfig = None,\n",
    "        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,\n",
    "        eval_dataset: Optional[\n",
    "            Union[Dataset, IterableDataset, dict[str, Union[Dataset, IterableDataset]]]\n",
    "        ] = None,\n",
    "        processing_class: Optional[PreTrainedTokenizerBase] = None,\n",
    "        reward_processing_classes: Optional[\n",
    "            Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]]\n",
    "        ] = None,\n",
    "        callbacks: Optional[list[TrainerCallback]] = None,\n",
    "        optimizers: tuple[\n",
    "            Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]\n",
    "        ] = (None, None),\n",
    "        peft_config: Optional[\"PeftConfig\"] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            reward_funcs=reward_funcs,\n",
    "            args=args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            processing_class=processing_class,\n",
    "            reward_processing_classes=reward_processing_classes,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=optimizers,\n",
    "            peft_config=peft_config\n",
    "        )\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if return_outputs:\n",
    "            raise ValueError(\"The GRPOTrainer does not support returning outputs\")\n",
    "\n",
    "        device = self.accelerator.device\n",
    "        prompts = [x[\"prompt\"] for x in inputs]\n",
    "        # prompts_text = [\n",
    "        #     maybe_apply_chat_template(example, self.processing_class)[\"prompt\"]\n",
    "        #     for example in inputs\n",
    "        # ]\n",
    "        prompt_inputs = self.processing_class(\n",
    "            prompts, #prompts_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            padding_side=\"left\",\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        prompt_inputs = super()._prepare_inputs(prompt_inputs)\n",
    "\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_inputs[\"input_ids\"] = prompt_inputs[\"input_ids\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "            prompt_inputs[\"attention_mask\"] = prompt_inputs[\"attention_mask\"][\n",
    "                :, -self.max_prompt_length :\n",
    "            ]\n",
    "\n",
    "        # Generate actions using the model\n",
    "        with unwrap_model_for_generation(model, self.accelerator) as unwrapped_model:\n",
    "            generated_actions = unwrapped_model.generate(\n",
    "                **prompt_inputs, num_return_sequences=self.num_generations\n",
    "            )\n",
    "\n",
    "        # Get the distribution parameters (mean, std) for the generated actions\n",
    "        def get_gaussian_params_and_logprobs(model, input_ids, actions):\n",
    "            \"\"\"\n",
    "            Get Gaussian distribution parameters and log probabilities for actions.\n",
    "\n",
    "            Args:\n",
    "                model: The policy model\n",
    "                input_ids: Input states (observations)\n",
    "                actions: Generated actions to compute log probs for\n",
    "\n",
    "            Returns:\n",
    "                mean, std, log_probs\n",
    "            \"\"\"\n",
    "            mean, std = model(input_ids.float())  # Get distribution parameters\n",
    "            # print(f\"mean {mean.shape}, std {std.shape}\")\n",
    "            dist = Normal(mean, std)\n",
    "\n",
    "            # raise ValueError(\"test\")\n",
    "            # Reshape actions to match batch size if needed\n",
    "            \n",
    "            # Handle num_generations > 1 case\n",
    "            actions = actions.view(-1, mean.shape[0], mean.shape[1])\n",
    "            # actions = actions.squeeze(1)  # Remove middle dimension if it's 1\n",
    "\n",
    "            log_probs = dist.log_prob(actions).sum(dim=-1)  # Sum over action dimensions\n",
    "            return mean, std, log_probs\n",
    "\n",
    "        # Get current policy distribution parameters and log probabilities\n",
    "        current_mean, current_std, current_log_probs = get_gaussian_params_and_logprobs(\n",
    "            model, prompt_inputs[\"input_ids\"], generated_actions\n",
    "        )\n",
    "\n",
    "        # Get reference policy distribution parameters and log probabilities\n",
    "        # with torch.inference_mode():\n",
    "        if self.ref_model is not None:\n",
    "            # print(f\"using ref model: {self.ref_model}\")\n",
    "            ref_mean, ref_std, ref_log_probs = get_gaussian_params_and_logprobs(\n",
    "                self.ref_model, prompt_inputs[\"input_ids\"], generated_actions\n",
    "            )\n",
    "        else:\n",
    "            # print(\"not using ref model\")\n",
    "            with self.accelerator.unwrap_model(model).disable_adapter():\n",
    "                ref_mean, ref_std, ref_log_probs = get_gaussian_params_and_logprobs(\n",
    "                    model, prompt_inputs[\"input_ids\"], generated_actions\n",
    "                )\n",
    "        ref_log_probs = ref_log_probs.clone()\n",
    "        ref_mean = ref_mean.clone()\n",
    "        ref_std = ref_std.clone()\n",
    "\n",
    "        kl_div = torch.distributions.kl.kl_divergence(\n",
    "                Normal(current_mean, current_std),\n",
    "                Normal(ref_mean, ref_std),\n",
    "        )\n",
    "        \n",
    "        # raise ValueError(\"kl_div shape: \", kl_div.shape)\n",
    "        kl_div = kl_div.sum(dim=-1)\n",
    "\n",
    "        # Decode the generated actions (convert back to list format for reward computation)\n",
    "        if generated_actions.dim() == 1:\n",
    "            generated_actions = generated_actions.unsqueeze(0)\n",
    "        completions = [action.cpu().numpy().tolist() for action in generated_actions]\n",
    "\n",
    "        # Compute the rewards\n",
    "        prompts_repeated = [\n",
    "            prompt for prompt in prompts for _ in range(self.num_generations)\n",
    "        ]\n",
    "\n",
    "        rewards_per_func = torch.zeros(\n",
    "            len(prompts_repeated), len(self.reward_funcs), device=device\n",
    "        )\n",
    "        for i, (reward_func, reward_processing_class) in enumerate(\n",
    "            zip(self.reward_funcs, self.reward_processing_classes)\n",
    "        ):\n",
    "            # Handle function-based rewards\n",
    "            reward_kwargs = {\n",
    "                key: []\n",
    "                for key in inputs[0].keys()\n",
    "                if key not in [\"prompt\", \"completion\"]\n",
    "            }\n",
    "            for key in reward_kwargs:\n",
    "                for example in inputs:\n",
    "                    reward_kwargs[key].extend([example[key]] * self.num_generations)\n",
    "            output_reward_func = reward_func(\n",
    "                prompts=prompts_repeated, completions=completions, **reward_kwargs\n",
    "            )\n",
    "            rewards_per_func[:, i] = torch.tensor(\n",
    "                output_reward_func, dtype=torch.float32, device=device\n",
    "            )\n",
    "\n",
    "        # Sum the rewards from all reward functions\n",
    "        rewards = rewards_per_func.sum(dim=1)\n",
    "\n",
    "        # Compute grouped-wise rewards (group by original prompt)\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "\n",
    "        # Normalize the rewards to compute advantages\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0\n",
    "        )\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        # Compute the policy gradient loss\n",
    "        # For continuous actions, we use the log probability ratio directly\n",
    "        ratio = torch.exp(current_log_probs - ref_log_probs.detach())\n",
    "        # raise ValueError(\"ratio shape: \", ratio.shape, \" advantages shape: \", advantages.shape, \" kl_div shape: \", kl_div.shape)\n",
    "        policy_loss = -(ratio * advantages.reshape(ratio.shape) - self.beta * kl_div)\n",
    "        loss = policy_loss.mean()\n",
    "\n",
    "        # Log the metrics\n",
    "        action_dim = generated_actions.shape[-1] if generated_actions.dim() > 1 else 1\n",
    "        self._metrics[\"action_dimension\"].append(action_dim)\n",
    "\n",
    "        reward_per_func = self.accelerator.gather_for_metrics(rewards_per_func).mean(0)\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            if isinstance(reward_func, PreTrainedModel):\n",
    "                reward_func_name = reward_func.config._name_or_path.split(\"/\")[-1]\n",
    "            else:\n",
    "                reward_func_name = reward_func.__name__\n",
    "            self._metrics[f\"rewards/{reward_func_name}\"].append(reward_per_func[i].item())\n",
    "\n",
    "        self._metrics[\"reward\"].append(\n",
    "            self.accelerator.gather_for_metrics(rewards).mean().item()\n",
    "        )\n",
    "        self._metrics[\"reward_std\"].append(\n",
    "            self.accelerator.gather_for_metrics(std_grouped_rewards).mean().item()\n",
    "        )\n",
    "        self._metrics[\"kl\"].append(\n",
    "            self.accelerator.gather_for_metrics(kl_div).mean().item()\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b89da4",
   "metadata": {},
   "source": [
    "### Reward fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1303fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(completions, state, action, reward, **kwargs):\n",
    "    rewards = []\n",
    "    for i, _ in enumerate(completions):\n",
    "        rewards.append(reward[i])\n",
    "    return rewards\n",
    "\n",
    "def BC_reward_fn(completions, state, action, reward, **kwargs):\n",
    "    rewards = []\n",
    "    for i, _ in enumerate(completions):\n",
    "        rewards.append(-np.linalg.norm(np.array(completions[i]) - np.array(action[i]), ord=2))\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fcf23",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6856c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"../../models/halfcheetah-mlp-grpo\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    beta=0.01,  # KL penalty coefficient\n",
    "    num_generations=4,  # number of actions to sample per state (for group advantage)\n",
    "    max_steps=10000,\n",
    ")\n",
    "\n",
    "# Create your model\n",
    "use_llm = False\n",
    "if not use_llm:\n",
    "    model = create_mlp_model(\n",
    "        obs_dim=input_dim, action_dim=output_dim, hidden_dims=[256, 256]\n",
    "    )\n",
    "    # Create tokenizer (if required)\n",
    "    tokenizer = MLPTokenizer(obs_dim=input_dim)\n",
    "else:\n",
    "    llm = \"Qwen/Qwen3-0.6B\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(llm)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm)\n",
    "\n",
    "trainer = CustomGRPOTrainer(\n",
    "    model=model,  # our MLP wrapped as PreTrainedModel\n",
    "    processing_class=tokenizer,  # dummy tokenizer\n",
    "    train_dataset=hf_dataset,  # dataset of states (and optionally a 'prompt' col)\n",
    "    reward_funcs=BC_reward_fn,  # a function returning list of rewards\n",
    "    args=training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76be5ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 13:29, Epoch 53/54]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>-0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-0.148700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>-0.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>-0.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>-0.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>-0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>-0.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>-0.154300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>-0.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>-0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>-0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>-0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>-0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>-0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>-0.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>-0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>-0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>-0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>-0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>-0.154300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10000, training_loss=-0.15066619873046874, metrics={'train_runtime': 809.6634, 'train_samples_per_second': 395.226, 'train_steps_per_second': 12.351, 'total_flos': 0.0, 'train_loss': -0.15066619873046874})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e8b623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_19357/4179726797.py\u001b[39m(\u001b[92m5\u001b[39m)\u001b[36mreward_fn\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      2\u001b[39m     rewards = []\n",
      "\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;28;01min\u001b[39;00m enumerate(completions):\n",
      "\u001b[32m      4\u001b[39m         rewards.append(reward[i])\n",
      "\u001b[32m----> 5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotImplementedError(\u001b[33m\"reward\"\u001b[39m)\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rewards\n",
      "\n",
      "64\n",
      "15.539128748929897\n",
      "15.539128748929897\n",
      "[15.539128748929897, 15.539128748929897, 15.539128748929897, 15.539128748929897, -0.7794300646493412]\n",
      "*** AttributeError: 'list' object has no attribute 'shape'\n",
      "64\n",
      "15.539128748929897\n",
      "[[0.9796957969665527, 0.6177719831466675, 0.9426043033599854, 0.976677417755127, -0.7610951066017151, 0.5502623319625854], [0.9796957969665527, 0.6177719831466675, 0.9426043033599854, 0.976677417755127, -0.7610951066017151, 0.5502623319625854], [0.9796957969665527, 0.6177719831466675, 0.9426043033599854, 0.976677417755127, -0.7610951066017151, 0.5502623319625854], [0.9796957969665527, 0.6177719831466675, 0.9426043033599854, 0.976677417755127, -0.7610951066017151, 0.5502623319625854], [-0.30068451166152954, -0.7801194190979004, -0.08793526887893677, 0.6079035997390747, -0.030026793479919434, 0.6677577495574951], [-0.30068451166152954, -0.7801194190979004, -0.08793526887893677, 0.6079035997390747, -0.030026793479919434, 0.6677577495574951], [-0.30068451166152954, -0.7801194190979004, -0.08793526887893677, 0.6079035997390747, -0.030026793479919434, 0.6677577495574951], [-0.30068451166152954, -0.7801194190979004, -0.08793526887893677, 0.6079035997390747, -0.030026793479919434, 0.6677577495574951], [0.506537914276123, -0.45583879947662354, 0.4859684705734253, -0.2705265283584595, 0.6678975820541382, 0.07155919075012207], [0.506537914276123, -0.45583879947662354, 0.4859684705734253, -0.2705265283584595, 0.6678975820541382, 0.07155919075012207], [0.506537914276123, -0.45583879947662354, 0.4859684705734253, -0.2705265283584595, 0.6678975820541382, 0.07155919075012207], [0.506537914276123, -0.45583879947662354, 0.4859684705734253, -0.2705265283584595, 0.6678975820541382, 0.07155919075012207], [0.9623289108276367, 0.22740209102630615, 0.9174456596374512, -0.967757523059845, -0.9828408360481262, -0.8996821045875549], [0.9623289108276367, 0.22740209102630615, 0.9174456596374512, -0.967757523059845, -0.9828408360481262, -0.8996821045875549], [0.9623289108276367, 0.22740209102630615, 0.9174456596374512, -0.967757523059845, -0.9828408360481262, -0.8996821045875549], [0.9623289108276367, 0.22740209102630615, 0.9174456596374512, -0.967757523059845, -0.9828408360481262, -0.8996821045875549], [0.9004729986190796, 0.8952021598815918, 0.7633492946624756, -0.1216084361076355, -0.9758378863334656, -0.675883412361145], [0.9004729986190796, 0.8952021598815918, 0.7633492946624756, -0.1216084361076355, -0.9758378863334656, -0.675883412361145], [0.9004729986190796, 0.8952021598815918, 0.7633492946624756, -0.1216084361076355, -0.9758378863334656, -0.675883412361145], [0.9004729986190796, 0.8952021598815918, 0.7633492946624756, -0.1216084361076355, -0.9758378863334656, -0.675883412361145], [0.9945892095565796, 0.6255733966827393, 0.9964073896408081, -0.23423713445663452, -0.615786612033844, 0.6930862665176392], [0.9945892095565796, 0.6255733966827393, 0.9964073896408081, -0.23423713445663452, -0.615786612033844, 0.6930862665176392], [0.9945892095565796, 0.6255733966827393, 0.9964073896408081, -0.23423713445663452, -0.615786612033844, 0.6930862665176392], [0.9945892095565796, 0.6255733966827393, 0.9964073896408081, -0.23423713445663452, -0.615786612033844, 0.6930862665176392], [0.1804642677307129, 0.7524539232254028, 0.2493903636932373, 0.906352162361145, -0.9695262312889099, -0.943296492099762], [0.1804642677307129, 0.7524539232254028, 0.2493903636932373, 0.906352162361145, -0.9695262312889099, -0.943296492099762], [0.1804642677307129, 0.7524539232254028, 0.2493903636932373, 0.906352162361145, -0.9695262312889099, -0.943296492099762], [0.1804642677307129, 0.7524539232254028, 0.2493903636932373, 0.906352162361145, -0.9695262312889099, -0.943296492099762], [-0.9751650094985962, -0.9809605479240417, -0.6191915273666382, 0.16615819931030273, 0.7990368604660034, 0.7570028305053711], [-0.9751650094985962, -0.9809605479240417, -0.6191915273666382, 0.16615819931030273, 0.7990368604660034, 0.7570028305053711], [-0.9751650094985962, -0.9809605479240417, -0.6191915273666382, 0.16615819931030273, 0.7990368604660034, 0.7570028305053711], [-0.9751650094985962, -0.9809605479240417, -0.6191915273666382, 0.16615819931030273, 0.7990368604660034, 0.7570028305053711], [0.966702938079834, -0.63924241065979, 0.9386802911758423, 0.9022061824798584, -0.7149887084960938, 0.9201092720031738], [0.966702938079834, -0.63924241065979, 0.9386802911758423, 0.9022061824798584, -0.7149887084960938, 0.9201092720031738], [0.966702938079834, -0.63924241065979, 0.9386802911758423, 0.9022061824798584, -0.7149887084960938, 0.9201092720031738], [0.966702938079834, -0.63924241065979, 0.9386802911758423, 0.9022061824798584, -0.7149887084960938, 0.9201092720031738], [0.9613453149795532, 0.3183525800704956, 0.9465067386627197, 0.9639832973480225, -0.8450465202331543, 0.4391477108001709], [0.9613453149795532, 0.3183525800704956, 0.9465067386627197, 0.9639832973480225, -0.8450465202331543, 0.4391477108001709], [0.9613453149795532, 0.3183525800704956, 0.9465067386627197, 0.9639832973480225, -0.8450465202331543, 0.4391477108001709], [0.9613453149795532, 0.3183525800704956, 0.9465067386627197, 0.9639832973480225, -0.8450465202331543, 0.4391477108001709], [0.9870972633361816, 0.7159996032714844, 0.9047517776489258, 0.9550015926361084, -0.9638224840164185, -0.9212208390235901], [0.9870972633361816, 0.7159996032714844, 0.9047517776489258, 0.9550015926361084, -0.9638224840164185, -0.9212208390235901], [0.9870972633361816, 0.7159996032714844, 0.9047517776489258, 0.9550015926361084, -0.9638224840164185, -0.9212208390235901], [0.9870972633361816, 0.7159996032714844, 0.9047517776489258, 0.9550015926361084, -0.9638224840164185, -0.9212208390235901], [0.4629383087158203, -0.45895904302597046, -0.9773175716400146, -0.8161559104919434, -0.9539958238601685, -0.825671374797821], [0.4629383087158203, -0.45895904302597046, -0.9773175716400146, -0.8161559104919434, -0.9539958238601685, -0.825671374797821], [0.4629383087158203, -0.45895904302597046, -0.9773175716400146, -0.8161559104919434, -0.9539958238601685, -0.825671374797821], [0.4629383087158203, -0.45895904302597046, -0.9773175716400146, -0.8161559104919434, -0.9539958238601685, -0.825671374797821], [0.9332406520843506, 0.9773193597793579, 0.8653738498687744, -0.6819479465484619, -0.8984014391899109, -0.784581184387207], [0.9332406520843506, 0.9773193597793579, 0.8653738498687744, -0.6819479465484619, -0.8984014391899109, -0.784581184387207], [0.9332406520843506, 0.9773193597793579, 0.8653738498687744, -0.6819479465484619, -0.8984014391899109, -0.784581184387207], [0.9332406520843506, 0.9773193597793579, 0.8653738498687744, -0.6819479465484619, -0.8984014391899109, -0.784581184387207], [0.42940282821655273, -0.9372552037239075, -0.935217022895813, -0.9397006034851074, -0.9521785378456116, -0.9453819394111633], [0.42940282821655273, -0.9372552037239075, -0.935217022895813, -0.9397006034851074, -0.9521785378456116, -0.9453819394111633], [0.42940282821655273, -0.9372552037239075, -0.935217022895813, -0.9397006034851074, -0.9521785378456116, -0.9453819394111633], [0.42940282821655273, -0.9372552037239075, -0.935217022895813, -0.9397006034851074, -0.9521785378456116, -0.9453819394111633], [-0.987983763217926, -0.8803130388259888, -0.8501791954040527, 0.2171025276184082, 0.9482531547546387, 0.929257869720459], [-0.987983763217926, -0.8803130388259888, -0.8501791954040527, 0.2171025276184082, 0.9482531547546387, 0.929257869720459], [-0.987983763217926, -0.8803130388259888, -0.8501791954040527, 0.2171025276184082, 0.9482531547546387, 0.929257869720459], [-0.987983763217926, -0.8803130388259888, -0.8501791954040527, 0.2171025276184082, 0.9482531547546387, 0.929257869720459], [0.9340369701385498, 0.9333668947219849, 0.9552847146987915, 0.7187726497650146, -0.8712249398231506, -0.9040371775627136], [0.9340369701385498, 0.9333668947219849, 0.9552847146987915, 0.7187726497650146, -0.8712249398231506, -0.9040371775627136], [0.9340369701385498, 0.9333668947219849, 0.9552847146987915, 0.7187726497650146, -0.8712249398231506, -0.9040371775627136], [0.9340369701385498, 0.9333668947219849, 0.9552847146987915, 0.7187726497650146, -0.8712249398231506, -0.9040371775627136]]\n",
      "64\n",
      "[0.9796957969665527, 0.6177719831466675, 0.9426043033599854, 0.976677417755127, -0.7610951066017151, 0.5502623319625854]\n",
      "[-0.30068451166152954, -0.7801194190979004, -0.08793526887893677, 0.6079035997390747, -0.030026793479919434, 0.6677577495574951]\n",
      "[0.9796957969665527, 0.6177719831466675, 0.9426043033599854, 0.976677417755127, -0.7610951066017151, 0.5502623319625854]\n",
      "*** AttributeError: 'list' object has no attribute 'shape'\n",
      "*** AttributeError: 'list' object has no attribute 'shape'\n",
      "64\n",
      "64\n",
      "[0.19401879608631134, 2.1613736152648926, -0.15090756118297577, 0.8490601181983948, -1.9714288711547852, 0.9559641480445862]\n",
      "[-0.6494408249855042, -0.8175247311592102, 1.1242578029632568, -1.2753498554229736, -2.1875460147857666, -0.3526875972747803]\n",
      "[-0.32603731751441956, -1.1599626541137695, 5.07003927230835, -0.6924470663070679, 0.18629556894302368, -1.917135238647461]\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15235c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
